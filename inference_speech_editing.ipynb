{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SiWiiUpv5iQg"
      },
      "outputs": [],
      "source": [
        "# import libs\n",
        "import torch\n",
        "import torchaudio\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "os.environ[\"USER\"] = \"YOUR_USERNAME\" # TODO change this to your username\n",
        "\n",
        "from data.tokenizer import (\n",
        "    AudioTokenizer,\n",
        "    TextTokenizer,\n",
        ")\n",
        "\n",
        "from models import voicecraft\n",
        "from edit_utils import parse_edit, get_edits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "a0pIv_pA5k0C"
      },
      "outputs": [],
      "source": [
        "# hyperparameters for inference\n",
        "left_margin = 0.08\n",
        "right_margin = 0.08\n",
        "sub_amount = 0.01\n",
        "codec_audio_sr = 16000\n",
        "codec_sr = 50\n",
        "top_k = 0\n",
        "top_p = 0.8\n",
        "temperature = 1\n",
        "kvcache = 0\n",
        "# NOTE: adjust the below three arguments if the generation is not as good\n",
        "seed = 1 # random seed magic\n",
        "silence_tokens = [1388,1898,131]\n",
        "stop_repetition = -1 # if there are long silence in the generated audio, reduce the stop_repetition to 3, 2 or even 1\n",
        "# what this will do to the model is that the model will run sample_batch_size examples of the same audio, and pick the one that's the shortest\n",
        "def seed_everything(seed):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "seed_everything(seed)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# point to the original file or record the file\n",
        "# write down the transcript for the file, or run whisper to get the transcript (and you can modify it if it's not accurate), save it as a .txt file\n",
        "orig_audio = \"./demo/84_121550_000074_000000.wav\"\n",
        "orig_transcript = \"But when I had approached so near to them The common object, which the sense deceives, Lost not by distance any of its marks,\"\n",
        "# move the audio and transcript to temp folder\n",
        "temp_folder = \"./demo/temp\"\n",
        "os.makedirs(temp_folder, exist_ok=True)\n",
        "os.system(f\"cp {orig_audio} {temp_folder}\")\n",
        "filename = os.path.splitext(orig_audio.split(\"/\")[-1])[0]\n",
        "with open(f\"{temp_folder}/{filename}.txt\", \"w\") as f:\n",
        "    f.write(orig_transcript)\n",
        "# run MFA to get the alignment\n",
        "align_temp = f\"{temp_folder}/mfa_alignments\"\n",
        "os.makedirs(align_temp, exist_ok=True)\n",
        "os.system(f\"mfa align -j 1 --clean --output_format csv {temp_folder} english_us_arpa english_us_arpa {align_temp}\")\n",
        "# if it fail, it could be because the audio is too hard for the alignment model, increasing the beam size usually solves the issue\n",
        "# os.system(f\"mfa align -j 1 --clean --output_format csv {temp_folder} english_us_arpa english_us_arpa {align_temp} --beam 1000 --retry_beam 2000\")\n",
        "audio_fn = f\"{temp_folder}/{filename}.wav\"\n",
        "transcript_fn = f\"{temp_folder}/{filename}.txt\"\n",
        "align_fn = f\"{align_temp}/{filename}.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iIPNTtibF4OL"
      },
      "outputs": [],
      "source": [
        "def get_mask_interval(ali_fn, word_span_ind, editType):\n",
        "    with open(ali_fn, \"r\") as rf:\n",
        "        data = [l.strip().split(\",\") for l in rf.readlines()]\n",
        "        data = data[1:]\n",
        "    tmp = word_span_ind.split(\",\")\n",
        "    s, e = int(tmp[0]), int(tmp[-1])\n",
        "    start = None\n",
        "    for j, item in enumerate(data):\n",
        "        if j == s and item[3] == \"words\":\n",
        "            if editType == 'insertion':\n",
        "                start = float(item[1])\n",
        "            else:\n",
        "                start = float(item[0])\n",
        "        if j == e and item[3] == \"words\":\n",
        "            if editType == 'insertion':\n",
        "                end = float(item[0])\n",
        "            else:\n",
        "                end = float(item[1])\n",
        "            assert start != None\n",
        "            break\n",
        "    return (start, end)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "krbq1mBM6GDE",
        "outputId": "d9267aef-05b2-4276-ee8b-5687cab5c612"
      },
      "outputs": [],
      "source": [
        "# propose what do you want the target modified transcript to be\n",
        "orig_transcript = \"But when I had approached so near to them which the sense deceives, Lost not by distance any of its marks,\"\n",
        "target_transcript = \"But I did approached so near to them which the sense deceives, Lost not by distance any of its marks,\"\n",
        "\n",
        "# from edit_utils import parse_edit, get_edits\n",
        "\n",
        "# run the script to turn user input to the format that the model can take\n",
        "operations, orig_span, new_span = parse_edit(orig_transcript, target_transcript)\n",
        "\n",
        "used_edits = get_edits(operations)\n",
        "print(used_edits) \n",
        "\n",
        "def process_span(span):\n",
        "    if span[0] > span[1]:\n",
        "        raise RuntimeError(f\"example {audio_fn} failed\")\n",
        "    if span[0] == span[1]:\n",
        "        return [span[0]]\n",
        "    return span\n",
        "\n",
        "print(\"orig_span: \", orig_span)\n",
        "print(\"new_span: \", new_span)\n",
        "orig_span_save = [process_span(span) for span in orig_span]\n",
        "new_span_save = [process_span(span) for span in new_span]\n",
        "\n",
        "orig_span_saves = [\",\".join([str(item) for item in span]) for span in orig_span_save]\n",
        "new_span_saves = [\",\".join([str(item) for item in span]) for span in new_span_save]\n",
        "\n",
        "starting_intervals = []\n",
        "ending_intervals = []\n",
        "for i, orig_span_save in enumerate(orig_span_saves):\n",
        "  start, end = get_mask_interval(align_fn, orig_span_save, used_edits[i])\n",
        "  starting_intervals.append(start)\n",
        "  ending_intervals.append(end)\n",
        "\n",
        "info = torchaudio.info(audio_fn)\n",
        "audio_dur = info.num_frames / info.sample_rate\n",
        "\n",
        "def resolve_overlap(starting_intervals, ending_intervals, audio_dur, codec_sr, left_margin, right_margin, sub_amount):\n",
        "    while True:\n",
        "        morphed_span = [(max(start - left_margin, 1/codec_sr), min(end + right_margin, audio_dur))\n",
        "                        for start, end in zip(starting_intervals, ending_intervals)] # in seconds\n",
        "        mask_interval = [[round(span[0]*codec_sr), round(span[1]*codec_sr)] for span in morphed_span]\n",
        "        # Check for overlap\n",
        "        overlapping = any(a[1] >= b[0] for a, b in zip(mask_interval, mask_interval[1:]))\n",
        "        if not overlapping:\n",
        "            break\n",
        "        \n",
        "        # Reduce margins\n",
        "        left_margin -= sub_amount\n",
        "        right_margin -= sub_amount\n",
        "    \n",
        "    return mask_interval\n",
        "\n",
        "\n",
        "# span in codec frames\n",
        "mask_interval = resolve_overlap(starting_intervals, ending_intervals, audio_dur, codec_sr, left_margin, right_margin, sub_amount)\n",
        "mask_interval = torch.LongTensor(mask_interval) # [M,2], M==1 for now\n",
        "\n",
        "# load model, tokenizer, and other necessary files\n",
        "voicecraft_name=\"giga330M.pth\" # or giga830M.pth, or the newer models at https://huggingface.co/pyp1/VoiceCraft/tree/main\n",
        "ckpt_fn =f\"./pretrained_models/{voicecraft_name}\"\n",
        "encodec_fn = \"./pretrained_models/encodec_4cb2048_giga.th\"\n",
        "if not os.path.exists(ckpt_fn):\n",
        "    os.system(f\"wget https://huggingface.co/pyp1/VoiceCraft/resolve/main/{voicecraft_name}\\?download\\=true\")\n",
        "    os.system(f\"mv {voicecraft_name}\\?download\\=true ./pretrained_models/{voicecraft_name}\")\n",
        "if not os.path.exists(encodec_fn):\n",
        "    os.system(f\"wget https://huggingface.co/pyp1/VoiceCraft/resolve/main/encodec_4cb2048_giga.th\")\n",
        "    os.system(f\"mv encodec_4cb2048_giga.th ./pretrained_models/encodec_4cb2048_giga.th\")\n",
        "ckpt = torch.load(ckpt_fn, map_location=\"cpu\")\n",
        "model = voicecraft.VoiceCraft(ckpt[\"config\"])\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "phn2num = ckpt['phn2num']\n",
        "\n",
        "text_tokenizer = TextTokenizer(backend=\"espeak\")\n",
        "audio_tokenizer = AudioTokenizer(signature=encodec_fn) # will also put the neural codec model on gpu\n",
        "\n",
        "# run the model to get the output\n",
        "from inference_speech_editing_scale import inference_one_sample\n",
        "\n",
        "decode_config = {'top_k': top_k, 'top_p': top_p, 'temperature': temperature, 'stop_repetition': stop_repetition, 'kvcache': kvcache, \"codec_audio_sr\": codec_audio_sr, \"codec_sr\": codec_sr, \"silence_tokens\": silence_tokens}\n",
        "orig_audio, new_audio = inference_one_sample(model, ckpt[\"config\"], phn2num, text_tokenizer, audio_tokenizer, audio_fn, target_transcript, mask_interval, device, decode_config)\n",
        "\n",
        "# save segments for comparison\n",
        "orig_audio, new_audio = orig_audio[0].cpu(), new_audio[0].cpu()\n",
        "# logging.info(f\"length of the resynthesize orig audio: {orig_audio.shape}\")\n",
        "\n",
        "# display the audio\n",
        "from IPython.display import Audio\n",
        "print(\"original:\")\n",
        "display(Audio(orig_audio, rate=codec_audio_sr))\n",
        "\n",
        "print(\"edited:\")\n",
        "display(Audio(new_audio, rate=codec_audio_sr))\n",
        "\n",
        "# # save the audio\n",
        "# # output_dir\n",
        "# output_dir = \"./demo/generated_se\"\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# save_fn_new = f\"{output_dir}/{os.path.basename(audio_fn)[:-4]}_new_seed{seed}.wav\"\n",
        "\n",
        "# torchaudio.save(save_fn_new, new_audio, codec_audio_sr)\n",
        "\n",
        "# save_fn_orig = f\"{output_dir}/{os.path.basename(audio_fn)[:-4]}_orig.wav\"\n",
        "# if not os.path.isfile(save_fn_orig):\n",
        "#     orig_audio, orig_sr = torchaudio.load(audio_fn)\n",
        "#     if orig_sr != codec_audio_sr:\n",
        "#         orig_audio = torchaudio.transforms.Resample(orig_sr, codec_audio_sr)(orig_audio)\n",
        "#     torchaudio.save(save_fn_orig, orig_audio, codec_audio_sr)\n",
        "\n",
        "# # if you get error importing T5 in transformers\n",
        "# # try\n",
        "# # pip uninstall Pillow\n",
        "# # pip install Pillow\n",
        "# # you are likely to get warning looks like WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1), this can be safely ignored"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
